#!/usr/bin/env python3
"""
OpenClaw Enhanced Memory System V3.5
çœŸå‘é‡è¯­ä¹‰æœç´¢ + BM25 æ··åˆæ£€ç´¢ + Cross-Encoder é‡æ’åº

V3.0 â†’ V3.5 å‡çº§ï¼š
1. Cross-Encoder é‡æ’åºï¼ˆqwen3-rerankï¼‰â€”â€” ç²¾åº¦æå‡ 20-30%
2. å¤šèŒƒå›´éš”ç¦»ï¼ˆscopeï¼‰â€”â€” ä¸åŒ agent æ‹¥æœ‰ç‹¬ç«‹è®°å¿†ç©ºé—´
3. å™ªéŸ³è¿‡æ»¤ â€”â€” è‡ªåŠ¨è¿‡æ»¤æ— æ„ä¹‰çŸ­è¯­
4. æ—¶é—´è¡°å‡ â€”â€” è¿‘æœŸè®°å¿†æƒé‡æ›´é«˜
"""

import json
import re
import time
import math
import os
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from collections import deque
from pathlib import Path
from dataclasses import dataclass, field

# å¯¼å…¥å¤šä¾›åº”å•† Embedding + Reranker
from embedding_provider import (
    MultiProviderEmbedding, cosine_similarity, EmbeddingResult,
    DashScopeReranker
)


# ========== å™ªéŸ³è¿‡æ»¤å™¨ ==========

class NoiseFilter:
    """
    å™ªéŸ³è¿‡æ»¤å™¨
    è‡ªåŠ¨è¿‡æ»¤æ— æ„ä¹‰ä¿¡æ¯ï¼Œé¿å…å­˜å…¥åƒåœ¾è®°å¿†

    è§„åˆ™ï¼š
    - å†…å®¹è¿‡çŸ­ï¼ˆ< 4 å­—ï¼‰
    - çº¯ç²¹çš„é—®å€™è¯­ã€æ„Ÿå¹è¯
    - çº¯æ ‡ç‚¹/è¡¨æƒ…
    - é‡å¤å†…å®¹
    """

    # å¸¸è§å™ªéŸ³çŸ­è¯­ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
    NOISE_PATTERNS = [
        r'^(ä½ å¥½|hello|hi|hey|å—¨|å“ˆå–½|ok|å¥½çš„|è°¢è°¢|thanks|å—¯|å“¦|å•Š|å‘¢|å§|äº†|æ˜¯çš„|å¯¹|æ²¡é”™|è¡Œ|å¯ä»¥)[\s!ï¼.ã€‚~ï¼Ÿ?]*$',
        r'^[\s\.\,\!\?\;\:\-\~\â€¦\ã€‚\ï¼Œ\ï¼\ï¼Ÿ\ï¼›\ï¼š]+$',  # çº¯æ ‡ç‚¹
        r'^[\U0001f600-\U0001f9ff\U00002702-\U000027b0\s]+$',  # çº¯è¡¨æƒ…
        r'^(lol|lmao|haha|å“ˆå“ˆ|å˜»å˜»|å‘µå‘µ|233+)\s*$',
    ]

    def __init__(self, min_length: int = 4):
        self.min_length = min_length
        self._compiled = [re.compile(p, re.IGNORECASE) for p in self.NOISE_PATTERNS]
        self._recent_hashes = deque(maxlen=200)  # å»é‡ç¼“å†²

    def is_noise(self, text: str) -> bool:
        """åˆ¤æ–­å†…å®¹æ˜¯å¦ä¸ºå™ªéŸ³"""
        text = text.strip()

        # è¿‡çŸ­
        if len(text) < self.min_length:
            return True

        # åŒ¹é…å™ªéŸ³æ¨¡å¼
        for pattern in self._compiled:
            if pattern.match(text):
                return True

        # é‡å¤å†…å®¹
        text_hash = hash(text[:100])
        if text_hash in self._recent_hashes:
            return True
        self._recent_hashes.append(text_hash)

        return False

    def filter_batch(self, texts: List[str]) -> List[str]:
        """æ‰¹é‡è¿‡æ»¤å™ªéŸ³"""
        return [t for t in texts if not self.is_noise(t)]


# ========== æ—¶é—´è¡°å‡ ==========

class TimeDecay:
    """
    æ—¶é—´è¡°å‡è®¡ç®—å™¨
    è¿‘æœŸè®°å¿†æƒé‡æ›´é«˜ï¼Œé¥è¿œè®°å¿†æƒé‡é™ä½

    ä½¿ç”¨æŒ‡æ•°è¡°å‡ï¼šscore * exp(-lambda * days_ago)
    - 1 å¤©å‰ï¼šæƒé‡ ~95%
    - 7 å¤©å‰ï¼šæƒé‡ ~70%
    - 30 å¤©å‰ï¼šæƒé‡ ~30%
    - 90 å¤©å‰ï¼šæƒé‡ ~10%
    """

    def __init__(self, half_life_days: float = 14.0):
        """
        Args:
            half_life_days: åŠè¡°æœŸï¼ˆå¤©æ•°ï¼‰ï¼Œå³å¤šå°‘å¤©åæƒé‡é™ä¸º 50%
        """
        # lambda = ln(2) / half_life
        self.decay_lambda = math.log(2) / half_life_days

    def apply(self, score: float, timestamp: str) -> float:
        """
        å¯¹åˆ†æ•°æ–½åŠ æ—¶é—´è¡°å‡

        Args:
            score: åŸå§‹åˆ†æ•°
            timestamp: ISO æ ¼å¼æ—¶é—´æˆ³
        """
        try:
            doc_time = datetime.fromisoformat(timestamp.replace("Z", "+00:00"))
            # å¤„ç† naive datetimeï¼ˆæ— æ—¶åŒºä¿¡æ¯ï¼‰
            now = datetime.now()
            if doc_time.tzinfo:
                now = datetime.now(doc_time.tzinfo)

            days_ago = max((now - doc_time).total_seconds() / 86400, 0)
            decay_factor = math.exp(-self.decay_lambda * days_ago)
            return score * decay_factor
        except Exception:
            # è§£æå¤±è´¥ä¸å½±å“è¯„åˆ†
            return score

    def get_decay_info(self, timestamp: str) -> Dict:
        """è·å–è¡°å‡è¯¦æƒ…ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
        try:
            doc_time = datetime.fromisoformat(timestamp.replace("Z", "+00:00"))
            now = datetime.now()
            if doc_time.tzinfo:
                now = datetime.now(doc_time.tzinfo)
            days_ago = max((now - doc_time).total_seconds() / 86400, 0)
            decay_factor = math.exp(-self.decay_lambda * days_ago)
            return {
                "days_ago": round(days_ago, 1),
                "decay_factor": round(decay_factor, 4)
            }
        except Exception:
            return {"days_ago": 0, "decay_factor": 1.0}


# ========== å‘é‡ç¼“å­˜ ==========

class VectorCache:
    """
    å‘é‡ç¼“å­˜ç®¡ç†å™¨
    å°†å·²è®¡ç®—çš„å‘é‡å­˜å…¥ JSON æ–‡ä»¶ï¼Œé¿å…é‡å¤è°ƒç”¨ Embedding API
    """

    def __init__(self, cache_path: str):
        self.cache_path = cache_path
        self.cache: Dict[str, List[float]] = {}
        self._load()

    def _load(self):
        if Path(self.cache_path).exists():
            try:
                with open(self.cache_path, "r", encoding="utf-8") as f:
                    self.cache = json.load(f)
            except Exception:
                self.cache = {}

    def _save(self):
        try:
            Path(self.cache_path).parent.mkdir(parents=True, exist_ok=True)
            with open(self.cache_path, "w", encoding="utf-8") as f:
                json.dump(self.cache, f, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸ å‘é‡ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

    @staticmethod
    def _text_key(text: str) -> str:
        return f"{hash(text[:200])}_{len(text)}"

    def get(self, text: str) -> Optional[List[float]]:
        return self.cache.get(self._text_key(text))

    def put(self, text: str, vector: List[float]):
        key = self._text_key(text)
        self.cache[key] = vector
        if len(self.cache) > 5000:
            keys = list(self.cache.keys())
            for old_key in keys[:1000]:
                del self.cache[old_key]
        self._save()


# ========== æœç´¢ç»“æœ ==========

@dataclass
class SearchResult:
    """æœç´¢ç»“æœ"""
    content: str
    score: float
    source: str
    timestamp: str
    scope: str = "default"
    metadata: Dict = field(default_factory=dict)
    rerank_score: Optional[float] = None
    time_decay: Optional[float] = None


# ========== æ··åˆæœç´¢å¼•æ“ V3.5 ==========

class HybridSearchEngine:
    """
    æ··åˆæœç´¢å¼•æ“ V3.5

    å®Œæ•´æµæ°´çº¿ï¼š
    1. å‘é‡è¯­ä¹‰æœç´¢ï¼ˆBi-Encoderï¼‰â€”â€” ç²—ç­›
    2. BM25 å…³é”®è¯æœç´¢ â€”â€” è¡¥å……ç²¾ç¡®åŒ¹é…
    3. æ··åˆèåˆ â€”â€” 70% å‘é‡ + 30% BM25
    4. æ—¶é—´è¡°å‡ â€”â€” è¿‘æœŸä¼˜å…ˆ
    5. Cross-Encoder é‡æ’åº â€”â€” ç²¾æ’ï¼ˆå¯é€‰ï¼‰
    """

    def __init__(self, embedder: MultiProviderEmbedding,
                 vector_cache: VectorCache,
                 reranker: Optional[DashScopeReranker] = None,
                 time_decay: Optional[TimeDecay] = None,
                 noise_filter: Optional[NoiseFilter] = None):
        self.embedder = embedder
        self.vector_cache = vector_cache
        self.reranker = reranker
        self.time_decay = time_decay or TimeDecay()
        self.noise_filter = noise_filter or NoiseFilter()

        # å¤šèŒƒå›´éš”ç¦»ï¼šæ¯ä¸ª scope æ‹¥æœ‰ç‹¬ç«‹çš„æ–‡æ¡£é›†
        self.scopes: Dict[str, Dict] = {}
        self._ensure_scope("default")

    def _ensure_scope(self, scope: str):
        """ç¡®ä¿ scope å­˜åœ¨"""
        if scope not in self.scopes:
            self.scopes[scope] = {
                "documents": [],
                "doc_vectors": [],
                "index": {},
                "idf_scores": {}
            }

    def add_document(self, doc_id: str, content: str,
                     metadata: Dict = None, vector: List[float] = None,
                     scope: str = "default",
                     skip_noise_filter: bool = False):
        """æ·»åŠ æ–‡æ¡£åˆ°æŒ‡å®š scope"""
        # å™ªéŸ³è¿‡æ»¤ï¼ˆå¯è·³è¿‡ï¼Œé¿å… add_memory å·²è¿‡æ»¤åé‡å¤æ£€æŸ¥ï¼‰
        if not skip_noise_filter and self.noise_filter.is_noise(content):
            return False

        self._ensure_scope(scope)
        s = self.scopes[scope]

        doc = {
            "id": doc_id,
            "content": content,
            "metadata": metadata or {},
            "tokens": self._tokenize(content),
            "timestamp": datetime.now().isoformat(),
            "scope": scope
        }
        s["documents"].append(doc)

        # è·å–æˆ–ç”Ÿæˆå‘é‡
        if vector is not None:
            s["doc_vectors"].append(vector)
        else:
            cached = self.vector_cache.get(content)
            if cached is not None:
                s["doc_vectors"].append(cached)
            else:
                try:
                    result = self.embedder.embed([content])
                    vec = result.vectors[0]
                    s["doc_vectors"].append(vec)
                    self.vector_cache.put(content, vec)
                except Exception as e:
                    print(f"âš ï¸ Embedding å¤±è´¥ (doc={doc_id}): {e}")
                    s["doc_vectors"].append([])

        # æ›´æ–° BM25 ç´¢å¼•
        for token in set(doc["tokens"]):
            if token not in s["index"]:
                s["index"][token] = []
            s["index"][token].append(doc["id"])
        s["idf_scores"] = {}  # é‡ç½® IDF
        return True

    def _tokenize(self, text: str) -> List[str]:
        """åˆ†è¯ï¼ˆä¸­è‹±æ–‡æ··åˆï¼‰"""
        text_lower = re.sub(r"[^\w\s]", " ", text.lower())
        tokens = text_lower.split()
        chinese_chars = re.findall(r"[\u4e00-\u9fff]+", text)
        for chars in chinese_chars:
            for i in range(len(chars)):
                tokens.append(chars[i])
                if i < len(chars) - 1:
                    tokens.append(chars[i:i+2])
        return tokens

    def _calculate_idf(self, scope_data: Dict):
        total_docs = max(len(scope_data["documents"]), 1)
        for token, doc_ids in scope_data["index"].items():
            scope_data["idf_scores"][token] = math.log(total_docs / len(doc_ids))

    def _bm25_score(self, query_tokens: List[str], doc: Dict,
                    scope_data: Dict) -> float:
        score = 0.0
        doc_tokens = doc["tokens"]
        doc_len = len(doc_tokens)
        docs = scope_data["documents"]
        avg_len = sum(len(d["tokens"]) for d in docs) / max(len(docs), 1)
        k1, b = 1.5, 0.75

        for token in query_tokens:
            if token in doc_tokens:
                tf = doc_tokens.count(token)
                idf = scope_data["idf_scores"].get(token, 0)
                norm = 1 - b + b * (doc_len / max(avg_len, 1))
                score += idf * (tf * (k1 + 1)) / (tf + k1 * norm)
        return score

    def hybrid_search(self, query: str, top_k: int = 5,
                      vector_weight: float = 0.7,
                      scope: str = "default",
                      enable_rerank: bool = True,
                      enable_time_decay: bool = True) -> List[SearchResult]:
        """
        æ··åˆæœç´¢ V3.5

        å®Œæ•´æµæ°´çº¿ï¼šç²—ç­› â†’ èåˆ â†’ æ—¶é—´è¡°å‡ â†’ ç²¾æ’
        """
        self._ensure_scope(scope)
        s = self.scopes[scope]

        if not s["documents"]:
            return []

        if not s["idf_scores"]:
            self._calculate_idf(s)

        # è·å–æŸ¥è¯¢å‘é‡
        query_vector = None
        try:
            cached = self.vector_cache.get(query)
            if cached is not None:
                query_vector = cached
            else:
                result = self.embedder.embed([query])
                query_vector = result.vectors[0]
                self.vector_cache.put(query, query_vector)
        except Exception as e:
            print(f"âš ï¸ æŸ¥è¯¢ Embedding å¤±è´¥: {e}ï¼Œé™çº§ä¸ºçº¯ BM25")
            vector_weight = 0.0

        query_tokens = self._tokenize(query)
        candidates = []

        # ç¬¬ 1-3 æ­¥ï¼šå‘é‡ + BM25 + èåˆ
        for i, doc in enumerate(s["documents"]):
            bm25 = self._bm25_score(query_tokens, doc, s)

            vec_score = 0.0
            if query_vector and i < len(s["doc_vectors"]) and s["doc_vectors"][i]:
                vec_score = cosine_similarity(query_vector, s["doc_vectors"][i])
                vec_score = (vec_score + 1) / 2

            final_score = vector_weight * vec_score + (1 - vector_weight) * bm25

            # ç¬¬ 4 æ­¥ï¼šæ—¶é—´è¡°å‡
            decay_factor = None
            if enable_time_decay and self.time_decay:
                decay_info = self.time_decay.get_decay_info(doc["timestamp"])
                decay_factor = decay_info["decay_factor"]
                final_score *= decay_factor

            candidates.append(SearchResult(
                content=doc["content"],
                score=final_score,
                source=doc["id"],
                timestamp=doc["timestamp"],
                scope=scope,
                metadata=doc.get("metadata", {}),
                time_decay=decay_factor
            ))

        # å…ˆæŒ‰æ··åˆåˆ†æ•°æ’åºï¼Œå– top-k * 2ï¼ˆç»™ rerank æ›´å¤šå€™é€‰ï¼‰
        candidates.sort(key=lambda x: x.score, reverse=True)
        rerank_pool = candidates[:top_k * 2]

        # ç¬¬ 5 æ­¥ï¼šCross-Encoder é‡æ’åº
        if (enable_rerank and self.reranker and
                self.reranker.is_available and len(rerank_pool) > 1):
            try:
                doc_texts = [r.content for r in rerank_pool]
                rerank_results = self.reranker.rerank(
                    query, doc_texts, top_n=top_k
                )
                # ç”¨ rerank åˆ†æ•°æ›¿æ¢æœ€ç»ˆæ’åº
                reranked = []
                for rr in rerank_results:
                    candidate = rerank_pool[rr.index]
                    candidate.rerank_score = rr.relevance_score
                    # æœ€ç»ˆåˆ†æ•° = 0.4 * ç²—ç­› + 0.6 * ç²¾æ’
                    candidate.score = 0.4 * candidate.score + 0.6 * rr.relevance_score
                    reranked.append(candidate)
                reranked.sort(key=lambda x: x.score, reverse=True)
                return reranked[:top_k]
            except Exception as e:
                print(f"âš ï¸ Rerank å¤±è´¥ï¼Œä½¿ç”¨ç²—ç­›ç»“æœ: {e}")

        return rerank_pool[:top_k]

    def get_scope_list(self) -> List[str]:
        """è·å–æ‰€æœ‰ scope åˆ—è¡¨"""
        return list(self.scopes.keys())

    def get_scope_stats(self, scope: str = "default") -> Dict:
        self._ensure_scope(scope)
        s = self.scopes[scope]
        return {
            "scope": scope,
            "documents": len(s["documents"]),
            "tokens": len(s["index"]),
            "vectors": len(s["doc_vectors"])
        }

    def get_stats(self) -> Dict:
        total_docs = sum(len(s["documents"]) for s in self.scopes.values())
        return {
            "total_documents": total_docs,
            "scopes": len(self.scopes),
            "scope_details": {
                name: len(s["documents"])
                for name, s in self.scopes.items()
            },
            "cached_vectors": len(self.vector_cache.cache),
            "reranker_available": (
                self.reranker.is_available if self.reranker else False
            ),
            "embedding_provider": self.embedder.get_stats()
        }


# ========== å¢å¼ºç‰ˆè®°å¿†ç³»ç»Ÿ V3.5 ==========

class EnhancedMemoryCore:
    """
    å¢å¼ºç‰ˆè®°å¿†æ ¸å¿ƒ V3.5

    V3.0 â†’ V3.5 æ–°å¢ï¼š
    1. Cross-Encoder é‡æ’åºï¼ˆç²¾åº¦ +20-30%ï¼‰
    2. å¤šèŒƒå›´éš”ç¦»ï¼ˆscope éš”ç¦»ä¸åŒ agent çš„è®°å¿†ï¼‰
    3. å™ªéŸ³è¿‡æ»¤ï¼ˆè‡ªåŠ¨è¿‡æ»¤æ— æ„ä¹‰çŸ­è¯­ï¼‰
    4. æ—¶é—´è¡°å‡ï¼ˆè¿‘æœŸè®°å¿†ä¼˜å…ˆï¼‰
    """

    def __init__(self, storage_path: str = "/root/.openclaw/memory/openclaw_memory_v3.json",
                 config_dir: str = None,
                 default_scope: str = "default",
                 half_life_days: float = 14.0):
        self.storage_path = storage_path
        self.default_scope = default_scope

        if config_dir is None:
            config_dir = os.path.dirname(os.path.abspath(__file__))

        config_path = os.path.join(config_dir, "embedding_config.json")
        cache_path = os.path.join(
            os.path.dirname(storage_path), "vector_cache.json"
        )

        # åŠ è½½é…ç½®
        with open(config_path, "r", encoding="utf-8") as f:
            config = json.load(f)

        # åˆå§‹åŒ–ç»„ä»¶
        self.embedder = MultiProviderEmbedding(config_path=config_path)
        self.vector_cache = VectorCache(cache_path)
        self.noise_filter = NoiseFilter()
        self.time_decay = TimeDecay(half_life_days=half_life_days)

        # åˆå§‹åŒ– Rerankerï¼ˆä½¿ç”¨ DashScope API Keyï¼‰
        ds_key = config.get("providers", {}).get("dashscope", {}).get("api_key", "")
        self.reranker = None
        if ds_key and not ds_key.startswith("YOUR_"):
            self.reranker = DashScopeReranker(api_key=ds_key)
            print("âœ… Cross-Encoder Reranker å·²å¯ç”¨ (qwen3-rerank)")

        # æ··åˆæœç´¢å¼•æ“
        self.search_engine = HybridSearchEngine(
            embedder=self.embedder,
            vector_cache=self.vector_cache,
            reranker=self.reranker,
            time_decay=self.time_decay,
            noise_filter=self.noise_filter
        )

        # åˆ†ç±»å­—å…¸
        self.context = {
            "session": {
                "current_id": None,
                "start_time": None,
                "message_count": 0
            },
            "user_profile": {
                "preferences": {},
                "expertise": {},
                "history_summary": deque(maxlen=50)
            },
            "knowledge_base": {
                "code_snippets": {},
                "documents": {},
                "concepts": {}
            },
            "tasks": {
                "active": deque(maxlen=10),
                "completed": deque(maxlen=20)
            },
            "conversation_log": deque(maxlen=100)
        }

        # ç»Ÿè®¡
        self.stats = {
            "searches": 0,
            "hits": 0,
            "noise_filtered": 0,
            "rerank_count": 0,
            "token_saved": 0
        }

        self.load()
        self._rebuild_search_index()

    def load(self):
        if Path(self.storage_path).exists():
            try:
                with open(self.storage_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                for key in ["user_profile", "tasks", "conversation_log"]:
                    if key in data:
                        if key == "conversation_log":
                            self.context[key] = deque(data[key], maxlen=100)
                        else:
                            for subkey, value in data[key].items():
                                if isinstance(value, list):
                                    max_len = 50 if "history" in subkey else 20
                                    self.context[key][subkey] = deque(
                                        value, maxlen=max_len
                                    )
                print("âœ… è®°å¿†åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å¤±è´¥: {e}")

    def save(self):
        try:
            Path(self.storage_path).parent.mkdir(parents=True, exist_ok=True)
            serializable = self._to_serializable(self.context)
            with open(self.storage_path, "w", encoding="utf-8") as f:
                json.dump(serializable, f, indent=2, ensure_ascii=False)
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            return False

    def _to_serializable(self, obj):
        if isinstance(obj, deque):
            return list(obj)
        elif isinstance(obj, dict):
            return {k: self._to_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._to_serializable(item) for item in obj]
        return obj

    def _rebuild_search_index(self):
        print("ğŸ”„ é‡å»ºæœç´¢ç´¢å¼•...")
        indexed = 0
        for i, msg in enumerate(self.context["conversation_log"]):
            if isinstance(msg, dict):
                content = msg.get("content", "")
                scope = msg.get("scope", self.default_scope)
                if content:
                    added = self.search_engine.add_document(
                        doc_id=f"conversation_{i}",
                        content=content,
                        metadata={
                            "role": msg.get("role"),
                            "timestamp": msg.get("timestamp")
                        },
                        scope=scope
                    )
                    if added:
                        indexed += 1

        for key, value in self.context["knowledge_base"].items():
            if isinstance(value, dict):
                for item_id, item_content in value.items():
                    content = str(item_content)
                    if content:
                        added = self.search_engine.add_document(
                            doc_id=f"knowledge_{key}_{item_id}",
                            content=content,
                            metadata={"category": key},
                            scope=self.default_scope
                        )
                        if added:
                            indexed += 1

        scopes = self.search_engine.get_scope_list()
        print(f"âœ… ç´¢å¼•é‡å»ºå®Œæˆ: {indexed} æ¡æ–‡æ¡£, {len(scopes)} ä¸ªèŒƒå›´")

    # ========== æ ¸å¿ƒåŠŸèƒ½ ==========

    def smart_recall(self, query: str, max_results: int = 5,
                     scope: str = None,
                     enable_rerank: bool = True) -> List[Dict]:
        """
        æ™ºèƒ½å›å¿†ï¼ˆV3.5 æ ¸å¿ƒï¼‰

        å®Œæ•´æµæ°´çº¿ï¼šå‘é‡æœç´¢ + BM25 + æ—¶é—´è¡°å‡ + Cross-Encoder ç²¾æ’
        """
        scope = scope or self.default_scope
        self.stats["searches"] += 1

        results = self.search_engine.hybrid_search(
            query, top_k=max_results, scope=scope,
            enable_rerank=enable_rerank
        )

        if results:
            self.stats["hits"] += 1
            if any(r.rerank_score is not None for r in results):
                self.stats["rerank_count"] += 1

            total_size = sum(
                len(str(msg)) for msg in self.context["conversation_log"]
            )
            retrieved_size = sum(len(r.content) for r in results)
            self.stats["token_saved"] += (total_size - retrieved_size) // 4

        return [
            {
                "content": r.content,
                "score": round(r.score, 4),
                "source": r.source,
                "timestamp": r.timestamp,
                "scope": r.scope,
                "rerank_score": round(r.rerank_score, 4) if r.rerank_score else None,
                "time_decay": round(r.time_decay, 4) if r.time_decay else None
            }
            for r in results
        ]

    def add_memory(self, content: str, category: str = "general",
                   metadata: Dict = None, scope: str = None):
        """æ·»åŠ æ–°è®°å¿†ï¼ˆè‡ªåŠ¨å™ªéŸ³è¿‡æ»¤ + åµŒå…¥ + ç´¢å¼•ï¼‰"""
        scope = scope or self.default_scope

        # å™ªéŸ³è¿‡æ»¤
        if self.noise_filter.is_noise(content):
            self.stats["noise_filtered"] += 1
            return False

        timestamp = datetime.now().isoformat()
        self.context["conversation_log"].append({
            "content": content,
            "category": category,
            "timestamp": timestamp,
            "scope": scope,
            "metadata": metadata or {}
        })

        doc_id = f"{category}_{len(self.context['conversation_log'])}"
        self.search_engine.add_document(
            doc_id, content, metadata, scope=scope,
            skip_noise_filter=True
        )
        self.save()
        return True

    def get_relevant_context(self, current_query: str,
                             max_tokens: int = 500,
                             scope: str = None) -> str:
        """è·å–ç›¸å…³ä¸Šä¸‹æ–‡"""
        relevant = self.smart_recall(
            current_query, max_results=3, scope=scope
        )
        if not relevant:
            return "ï¼ˆæ— ç›¸å…³å†å²è®°å½•ï¼‰"

        parts = ["=== ç›¸å…³è®°å¿† ==="]
        current_tokens = 0
        for mem in relevant:
            text = f"[{mem['source']}] {mem['content'][:200]}"
            tokens = len(text) // 4
            if current_tokens + tokens > max_tokens:
                break
            parts.append(text)
            current_tokens += tokens
        return "\n".join(parts)

    def get_memory_stats(self) -> Dict:
        search_stats = self.search_engine.get_stats()
        return {
            **search_stats,
            "total_conversations": len(self.context["conversation_log"]),
            "active_tasks": len(self.context["tasks"]["active"]),
            "noise_filtered": self.stats["noise_filtered"],
            "search_efficiency": {
                "total_searches": self.stats["searches"],
                "successful_hits": self.stats["hits"],
                "rerank_used": self.stats["rerank_count"],
                "hit_rate": (
                    f"{self.stats['hits'] / self.stats['searches'] * 100:.1f}%"
                    if self.stats["searches"] > 0 else "0%"
                ),
                "estimated_tokens_saved": self.stats["token_saved"]
            }
        }

    def print_stats(self):
        stats = self.get_memory_stats()
        print("\n" + "=" * 60)
        print("ğŸ“Š å¢å¼ºç‰ˆè®°å¿†ç³»ç»Ÿ V3.5 ç»Ÿè®¡")
        print("=" * 60)
        print(f"ğŸ’¾ å­˜å‚¨ç»Ÿè®¡:")
        print(f"  - æ–‡æ¡£æ€»æ•°: {stats['total_documents']}")
        print(f"  - ç¼“å­˜å‘é‡: {stats['cached_vectors']}")
        print(f"  - å¯¹è¯æ¡æ•°: {stats['total_conversations']}")
        print(f"  - èŒƒå›´æ•°é‡: {stats['scopes']}")
        for name, count in stats["scope_details"].items():
            print(f"    ğŸ“‚ {name}: {count} æ¡")
        print(f"\nğŸ” æœç´¢æ•ˆç‡:")
        eff = stats["search_efficiency"]
        print(f"  - æœç´¢æ¬¡æ•°: {eff['total_searches']}")
        print(f"  - å‘½ä¸­ç‡: {eff['hit_rate']}")
        print(f"  - Rerank æ¬¡æ•°: {eff['rerank_used']}")
        print(f"  - å™ªéŸ³è¿‡æ»¤: {stats['noise_filtered']} æ¡")
        print(f"  - ä¼°ç®—èŠ‚çœ Token: {eff['estimated_tokens_saved']:,}")
        print(f"\nğŸŒ ç»„ä»¶çŠ¶æ€:")
        for p in stats["embedding_provider"]["providers"]:
            status = "âœ…" if p["available"] else "âŒ"
            print(f"  {status} {p['name']} ({p['model']})")
        rr_status = "âœ…" if stats["reranker_available"] else "âŒ"
        print(f"  {rr_status} reranker (qwen3-rerank)")
        print("=" * 60 + "\n")


# ========== æ¼”ç¤º ==========

if __name__ == "__main__":
    print("ğŸš€ OpenClaw å¢å¼ºè®°å¿†ç³»ç»Ÿ V3.5 æ¼”ç¤º\n")

    memory = EnhancedMemoryCore(
        storage_path="/tmp/test_memory_v35.json",
        config_dir=os.path.dirname(os.path.abspath(__file__))
    )

    # â”€â”€ æµ‹è¯• 1: å™ªéŸ³è¿‡æ»¤ â”€â”€
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯• 1: å™ªéŸ³è¿‡æ»¤")
    print("=" * 60)
    noise_tests = ["ä½ å¥½", "ok", "ğŸ‘", "å—¯", "å“ˆå“ˆå“ˆ"]
    for noise in noise_tests:
        added = memory.add_memory(noise)
        print(f"  '{noise}' â†’ {'âŒ å·²è¿‡æ»¤' if not added else 'âœ… å­˜å…¥'}")

    # â”€â”€ æµ‹è¯• 2: å¤šèŒƒå›´éš”ç¦» â”€â”€
    print(f"\n{'=' * 60}")
    print("ğŸ§ª æµ‹è¯• 2: å¤šèŒƒå›´éš”ç¦»")
    print("=" * 60)
    memory.add_memory("ç”¨æˆ·å–œæ¬¢ç®€æ´çš„ä»£ç é£æ ¼", scope="personal")
    memory.add_memory("äº¤æ˜“æœºå™¨äººéœ€è¦ä¼˜åŒ–å»¶è¿Ÿ", scope="project-bot")
    memory.add_memory("ICLRè®ºæ–‡æˆªæ­¢æ—¥æœŸæ˜¯3æœˆ", scope="project-paper")
    memory.add_memory("OpenClawè®°å¿†ç³»ç»Ÿå·²å‡çº§åˆ°V3.5", scope="personal")
    print("  âœ… å·²å­˜å…¥ 3 ä¸ªä¸åŒ scope")

    # åªæœç´¢ç‰¹å®š scope
    print("\n  æœç´¢ scope='personal':")
    results = memory.smart_recall("ä»£ç è§„èŒƒ", scope="personal", enable_rerank=False)
    for r in results:
        print(f"    [{r['score']:.4f}] [{r['scope']}] {r['content'][:40]}")

    print("\n  æœç´¢ scope='project-bot':")
    results = memory.smart_recall("æ€§èƒ½ä¼˜åŒ–", scope="project-bot", enable_rerank=False)
    for r in results:
        print(f"    [{r['score']:.4f}] [{r['scope']}] {r['content'][:40]}")

    # â”€â”€ æµ‹è¯• 3: æ—¶é—´è¡°å‡ â”€â”€
    print(f"\n{'=' * 60}")
    print("ğŸ§ª æµ‹è¯• 3: æ—¶é—´è¡°å‡")
    print("=" * 60)
    decay = TimeDecay(half_life_days=14)
    for days in [0, 1, 7, 14, 30, 90]:
        ts = (datetime.now() - timedelta(days=days)).isoformat()
        info = decay.get_decay_info(ts)
        bar = "â–ˆ" * int(info["decay_factor"] * 20)
        print(f"  {days:3d} å¤©å‰: {info['decay_factor']:.3f} {bar}")

    # â”€â”€ æµ‹è¯• 4: Cross-Encoder é‡æ’åº â”€â”€
    print(f"\n{'=' * 60}")
    print("ğŸ§ª æµ‹è¯• 4: Cross-Encoder é‡æ’åº")
    print("=" * 60)
    memory.add_memory("Pythoné‡åŒ–äº¤æ˜“æœºå™¨äººä½¿ç”¨ccxtåº“", scope="default")
    memory.add_memory("è®¨è®ºäº†å¦‚ä½•ä¼˜åŒ–OpenClawçš„Tokenæ¶ˆè€—", scope="default")
    memory.add_memory("Vue.jså‰ç«¯æ¡†æ¶çš„ç»„ä»¶è®¾è®¡æ¨¡å¼", scope="default")
    memory.add_memory("æœåŠ¡å™¨éƒ¨ç½²ä½¿ç”¨Dockerå®¹å™¨åŒ–", scope="default")

    print("\n  æ—  Rerank:")
    results_no_rr = memory.smart_recall("ç¼–ç¨‹è¯­è¨€", enable_rerank=False)
    for r in results_no_rr[:3]:
        print(f"    [{r['score']:.4f}] {r['content'][:40]}")

    print("\n  æœ‰ Rerank:")
    results_rr = memory.smart_recall("ç¼–ç¨‹è¯­è¨€", enable_rerank=True)
    for r in results_rr[:3]:
        rr_str = f" (rerank={r['rerank_score']})" if r['rerank_score'] else ""
        print(f"    [{r['score']:.4f}]{rr_str} {r['content'][:40]}")

    # ç»Ÿè®¡
    print()
    memory.print_stats()
