#!/usr/bin/env python3
"""
OpenClaw Enhanced Memory System V3.0
çœŸå‘é‡è¯­ä¹‰æœç´¢ + BM25 æ··åˆæ£€ç´¢ + å¤šä¾›åº”å•† Embedding

å‡çº§è¦ç‚¹ï¼ˆV2 â†’ V3ï¼‰ï¼š
1. Jaccard è¯è¢‹åŒ¹é… â†’ ä½™å¼¦å‘é‡è¯­ä¹‰æœç´¢ï¼ˆçœŸ Embeddingï¼‰
2. é›†æˆ DashScope/Google/Jina ä¸‰ä¾›åº”å•†è‡ªåŠ¨ fallback
3. å‘é‡ç¼“å­˜ï¼ˆé¿å…é‡å¤è°ƒç”¨ APIï¼‰
4. ä¿ç•™ BM25 åšæ··åˆæ£€ç´¢
"""

import json
import re
import time
import math
import os
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from collections import deque
from pathlib import Path
from dataclasses import dataclass, field

# å¯¼å…¥å¤šä¾›åº”å•† Embedding
from embedding_provider import (
    MultiProviderEmbedding, cosine_similarity, EmbeddingResult
)


# ========== å‘é‡ç¼“å­˜ ==========

class VectorCache:
    """
    å‘é‡ç¼“å­˜ç®¡ç†å™¨
    å°†å·²è®¡ç®—çš„å‘é‡å­˜å…¥ JSON æ–‡ä»¶ï¼Œé¿å…é‡å¤è°ƒç”¨ Embedding API

    ç¼“å­˜ç­–ç•¥ï¼šåŸºäºæ–‡æœ¬å†…å®¹çš„ hash åšé”®
    """

    def __init__(self, cache_path: str):
        self.cache_path = cache_path
        self.cache: Dict[str, List[float]] = {}
        self._load()

    def _load(self):
        if Path(self.cache_path).exists():
            try:
                with open(self.cache_path, "r", encoding="utf-8") as f:
                    self.cache = json.load(f)
            except Exception:
                self.cache = {}

    def _save(self):
        try:
            Path(self.cache_path).parent.mkdir(parents=True, exist_ok=True)
            with open(self.cache_path, "w", encoding="utf-8") as f:
                json.dump(self.cache, f, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸ å‘é‡ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

    @staticmethod
    def _text_key(text: str) -> str:
        """ç”Ÿæˆæ–‡æœ¬çš„ç¼“å­˜é”®ï¼ˆç®€å• hashï¼‰"""
        # å–å‰ 200 å­—ç¬¦ + é•¿åº¦ä½œä¸ºé”®ï¼Œé¿å…è¶…é•¿é”®
        return f"{hash(text[:200])}_{len(text)}"

    def get(self, text: str) -> Optional[List[float]]:
        key = self._text_key(text)
        return self.cache.get(key)

    def put(self, text: str, vector: List[float]):
        key = self._text_key(text)
        self.cache[key] = vector
        # ç¼“å­˜è¶…è¿‡ 5000 æ¡æ—¶æ¸…ç†æœ€æ—©çš„ï¼ˆç®€å• FIFOï¼‰
        if len(self.cache) > 5000:
            keys = list(self.cache.keys())
            for old_key in keys[:1000]:
                del self.cache[old_key]
        self._save()

    def batch_get(self, texts: List[str]) -> Tuple[List[str], List[int], List[List[float]]]:
        """
        æ‰¹é‡è·å–ï¼šè¿”å›æœªç¼“å­˜çš„æ–‡æœ¬åˆ—è¡¨ã€å…¶ç´¢å¼•ã€ä»¥åŠå·²ç¼“å­˜çš„å‘é‡

        Returns:
            (uncached_texts, uncached_indices, cached_vectors_at_positions)
        """
        uncached_texts = []
        uncached_indices = []
        results = [None] * len(texts)

        for i, text in enumerate(texts):
            cached = self.get(text)
            if cached is not None:
                results[i] = cached
            else:
                uncached_texts.append(text)
                uncached_indices.append(i)

        return uncached_texts, uncached_indices, results


# ========== æœç´¢ç»“æœ ==========

@dataclass
class SearchResult:
    """æœç´¢ç»“æœ"""
    content: str
    score: float
    source: str
    timestamp: str
    metadata: Dict = field(default_factory=dict)


# ========== æ··åˆæœç´¢å¼•æ“ V3 ==========

class HybridSearchEngine:
    """
    æ··åˆæœç´¢å¼•æ“ V3

    æ ¸å¿ƒå‡çº§ï¼š
    - å‘é‡è¯­ä¹‰æœç´¢ï¼šä½¿ç”¨çœŸå® Embeddingï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
    - BM25 å…³é”®è¯æœç´¢ï¼šä¿ç•™ç”¨äºç²¾ç¡®åŒ¹é…
    - æ··åˆèåˆï¼š70% å‘é‡ + 30% BM25
    """

    def __init__(self, embedder: MultiProviderEmbedding,
                 vector_cache: VectorCache):
        self.embedder = embedder
        self.vector_cache = vector_cache
        self.documents = []
        self.doc_vectors = []  # æ¯ä¸ªæ–‡æ¡£å¯¹åº”çš„å‘é‡
        self.index = {}        # BM25 å€’æ’ç´¢å¼•
        self.idf_scores = {}

    def add_document(self, doc_id: str, content: str,
                     metadata: Dict = None, vector: List[float] = None):
        """æ·»åŠ æ–‡æ¡£ï¼ˆå¦‚æœæ²¡æœ‰å‘é‡åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰"""
        doc = {
            "id": doc_id,
            "content": content,
            "metadata": metadata or {},
            "tokens": self._tokenize(content),
            "timestamp": datetime.now().isoformat()
        }
        self.documents.append(doc)

        # è·å–æˆ–ç”Ÿæˆå‘é‡
        if vector is not None:
            self.doc_vectors.append(vector)
        else:
            cached = self.vector_cache.get(content)
            if cached is not None:
                self.doc_vectors.append(cached)
            else:
                try:
                    result = self.embedder.embed([content])
                    vec = result.vectors[0]
                    self.doc_vectors.append(vec)
                    self.vector_cache.put(content, vec)
                except Exception as e:
                    # Embedding å¤±è´¥æ—¶ç”¨é›¶å‘é‡å ä½ï¼Œä¸å½±å“ BM25
                    print(f"âš ï¸ Embedding å¤±è´¥ (doc={doc_id}): {e}")
                    self.doc_vectors.append([])

        # æ›´æ–° BM25 ç´¢å¼•
        self._update_index(doc)
        self.idf_scores = {}  # é‡ç½® IDF ç¼“å­˜

    def _tokenize(self, text: str) -> List[str]:
        """åˆ†è¯ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰"""
        text_lower = re.sub(r"[^\w\s]", " ", text.lower())
        tokens = text_lower.split()
        # ä¸­æ–‡æŒ‰å­—ç¬¦æ‹†åˆ†ï¼ˆå¯å‡çº§ä¸º jiebaï¼‰
        chinese_chars = re.findall(r"[\u4e00-\u9fff]+", text)
        for chars in chinese_chars:
            # æŒ‰ 2-gram æ‹†åˆ†ä»¥æå‡ä¸­æ–‡åŒ¹é…ç²¾åº¦
            for i in range(len(chars)):
                tokens.append(chars[i])
                if i < len(chars) - 1:
                    tokens.append(chars[i:i+2])
        return tokens

    def _update_index(self, doc: Dict):
        for token in set(doc["tokens"]):
            if token not in self.index:
                self.index[token] = []
            self.index[token].append(doc["id"])

    def _calculate_idf(self):
        total_docs = max(len(self.documents), 1)
        for token, doc_ids in self.index.items():
            self.idf_scores[token] = math.log(total_docs / len(doc_ids))

    def _bm25_score(self, query_tokens: List[str], doc: Dict) -> float:
        """BM25 è¯„åˆ†"""
        score = 0.0
        doc_tokens = doc["tokens"]
        doc_len = len(doc_tokens)
        avg_len = sum(len(d["tokens"]) for d in self.documents) / max(len(self.documents), 1)
        k1, b = 1.5, 0.75

        for token in query_tokens:
            if token in doc_tokens:
                tf = doc_tokens.count(token)
                idf = self.idf_scores.get(token, 0)
                norm = 1 - b + b * (doc_len / max(avg_len, 1))
                score += idf * (tf * (k1 + 1)) / (tf + k1 * norm)

        return score

    def hybrid_search(self, query: str, top_k: int = 5,
                      vector_weight: float = 0.7) -> List[SearchResult]:
        """
        æ··åˆæœç´¢

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å› top-k ç»“æœ
            vector_weight: å‘é‡æœç´¢æƒé‡ï¼ˆ0-1ï¼‰ï¼Œå‰©ä½™ä¸º BM25 æƒé‡
        """
        if not self.documents:
            return []

        # è®¡ç®— IDF
        if not self.idf_scores:
            self._calculate_idf()

        # è·å–æŸ¥è¯¢å‘é‡
        query_vector = None
        try:
            cached = self.vector_cache.get(query)
            if cached is not None:
                query_vector = cached
            else:
                result = self.embedder.embed([query])
                query_vector = result.vectors[0]
                self.vector_cache.put(query, query_vector)
        except Exception as e:
            print(f"âš ï¸ æŸ¥è¯¢ Embedding å¤±è´¥: {e}ï¼Œå°†ä»…ä½¿ç”¨ BM25")
            vector_weight = 0.0  # é™çº§ä¸ºçº¯ BM25

        query_tokens = self._tokenize(query)
        results = []

        for i, doc in enumerate(self.documents):
            # BM25 åˆ†æ•°
            bm25 = self._bm25_score(query_tokens, doc)

            # å‘é‡åˆ†æ•°
            vec_score = 0.0
            if query_vector and i < len(self.doc_vectors) and self.doc_vectors[i]:
                vec_score = cosine_similarity(query_vector, self.doc_vectors[i])
                # ä½™å¼¦ç›¸ä¼¼åº¦èŒƒå›´ [-1, 1]ï¼Œå½’ä¸€åŒ–åˆ° [0, 1]
                vec_score = (vec_score + 1) / 2

            # æ··åˆåˆ†æ•°
            final_score = vector_weight * vec_score + (1 - vector_weight) * bm25

            results.append(SearchResult(
                content=doc["content"],
                score=final_score,
                source=doc["id"],
                timestamp=doc["timestamp"],
                metadata=doc.get("metadata", {})
            ))

        results.sort(key=lambda x: x.score, reverse=True)
        return results[:top_k]

    def get_stats(self) -> Dict:
        return {
            "total_documents": len(self.documents),
            "unique_tokens": len(self.index),
            "cached_vectors": len(self.vector_cache.cache),
            "avg_doc_length": (
                sum(len(d["tokens"]) for d in self.documents) /
                max(len(self.documents), 1)
            ),
            "embedding_provider": self.embedder.get_stats()
        }


# ========== å¢å¼ºç‰ˆè®°å¿†ç³»ç»Ÿ V3 ==========

class EnhancedMemoryCore:
    """
    å¢å¼ºç‰ˆè®°å¿†æ ¸å¿ƒ V3

    V2 â†’ V3 å‡çº§ï¼š
    1. Jaccard è¯è¢‹ â†’ çœŸå‘é‡è¯­ä¹‰æœç´¢
    2. å¤šä¾›åº”å•† Embeddingï¼ˆDashScope/Google/Jinaï¼‰
    3. å‘é‡ç¼“å­˜ï¼ˆé¿å…é‡å¤ API è°ƒç”¨ï¼‰
    """

    def __init__(self, storage_path: str = "/root/.openclaw/memory/openclaw_memory_v3.json",
                 config_dir: str = None):
        self.storage_path = storage_path

        # ç¡®å®šé…ç½®ç›®å½•
        if config_dir is None:
            config_dir = os.path.dirname(os.path.abspath(__file__))

        config_path = os.path.join(config_dir, "embedding_config.json")
        cache_path = os.path.join(
            os.path.dirname(storage_path), "vector_cache.json"
        )

        # åˆå§‹åŒ– Embedding ç®¡ç†å™¨
        self.embedder = MultiProviderEmbedding(config_path=config_path)

        # åˆå§‹åŒ–å‘é‡ç¼“å­˜
        self.vector_cache = VectorCache(cache_path)

        # åˆ†ç±»å­—å…¸ï¼ˆä¿ç•™ V2 çš„ç»“æ„ï¼‰
        self.context = {
            "session": {
                "current_id": None,
                "start_time": None,
                "message_count": 0
            },
            "user_profile": {
                "preferences": {},
                "expertise": {},
                "history_summary": deque(maxlen=50)
            },
            "knowledge_base": {
                "code_snippets": {},
                "documents": {},
                "concepts": {}
            },
            "tasks": {
                "active": deque(maxlen=10),
                "completed": deque(maxlen=20)
            },
            "conversation_log": deque(maxlen=100)
        }

        # æ··åˆæœç´¢å¼•æ“ï¼ˆV3 æ ¸å¿ƒï¼ï¼‰
        self.search_engine = HybridSearchEngine(
            embedder=self.embedder,
            vector_cache=self.vector_cache
        )

        # ç»Ÿè®¡
        self.stats = {
            "searches": 0,
            "hits": 0,
            "token_saved": 0
        }

        self.load()
        self._rebuild_search_index()

    def load(self):
        """åŠ è½½è®°å¿†"""
        if Path(self.storage_path).exists():
            try:
                with open(self.storage_path, "r", encoding="utf-8") as f:
                    data = json.load(f)

                for key in ["user_profile", "tasks", "conversation_log"]:
                    if key in data:
                        if key == "conversation_log":
                            self.context[key] = deque(data[key], maxlen=100)
                        else:
                            for subkey, value in data[key].items():
                                if isinstance(value, list):
                                    max_len = 50 if "history" in subkey else 20
                                    self.context[key][subkey] = deque(
                                        value, maxlen=max_len
                                    )

                print("âœ… è®°å¿†åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å¤±è´¥: {e}")

    def save(self):
        """ä¿å­˜è®°å¿†"""
        try:
            Path(self.storage_path).parent.mkdir(parents=True, exist_ok=True)
            serializable = self._to_serializable(self.context)
            with open(self.storage_path, "w", encoding="utf-8") as f:
                json.dump(serializable, f, indent=2, ensure_ascii=False)
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            return False

    def _to_serializable(self, obj):
        if isinstance(obj, deque):
            return list(obj)
        elif isinstance(obj, dict):
            return {k: self._to_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._to_serializable(item) for item in obj]
        return obj

    def _rebuild_search_index(self):
        """é‡å»ºæœç´¢ç´¢å¼•ï¼ˆå¯åŠ¨æ—¶æ‰§è¡Œï¼Œä½¿ç”¨ç¼“å­˜å‘é‡é¿å…é‡å¤ API è°ƒç”¨ï¼‰"""
        print("ğŸ”„ é‡å»ºæœç´¢ç´¢å¼•...")
        indexed = 0

        for i, msg in enumerate(self.context["conversation_log"]):
            if isinstance(msg, dict):
                content = msg.get("content", "")
                if content:
                    self.search_engine.add_document(
                        doc_id=f"conversation_{i}",
                        content=content,
                        metadata={
                            "role": msg.get("role"),
                            "timestamp": msg.get("timestamp")
                        }
                    )
                    indexed += 1

        for key, value in self.context["knowledge_base"].items():
            if isinstance(value, dict):
                for item_id, item_content in value.items():
                    content = str(item_content)
                    if content:
                        self.search_engine.add_document(
                            doc_id=f"knowledge_{key}_{item_id}",
                            content=content,
                            metadata={"category": key}
                        )
                        indexed += 1

        print(f"âœ… ç´¢å¼•é‡å»ºå®Œæˆ: {indexed} æ¡æ–‡æ¡£")

    # ========== æ ¸å¿ƒåŠŸèƒ½ ==========

    def smart_recall(self, query: str, max_results: int = 5) -> List[Dict]:
        """
        æ™ºèƒ½å›å¿†ï¼ˆV3 æ ¸å¿ƒï¼‰

        ä½¿ç”¨çœŸå‘é‡è¯­ä¹‰æœç´¢ + BM25 æ··åˆæ£€ç´¢
        èƒ½ç†è§£åŒä¹‰è¯å’Œè¯­ä¹‰ç›¸è¿‘çš„è¡¨è¿°
        """
        self.stats["searches"] += 1
        results = self.search_engine.hybrid_search(query, top_k=max_results)

        if results:
            self.stats["hits"] += 1
            total_size = sum(
                len(str(msg)) for msg in self.context["conversation_log"]
            )
            retrieved_size = sum(len(r.content) for r in results)
            self.stats["token_saved"] += (total_size - retrieved_size) // 4

        return [
            {
                "content": r.content,
                "score": r.score,
                "source": r.source,
                "timestamp": r.timestamp
            }
            for r in results
        ]

    def add_memory(self, content: str, category: str = "general",
                   metadata: Dict = None):
        """æ·»åŠ æ–°è®°å¿†ï¼ˆè‡ªåŠ¨åµŒå…¥ + ç´¢å¼•ï¼‰"""
        timestamp = datetime.now().isoformat()

        self.context["conversation_log"].append({
            "content": content,
            "category": category,
            "timestamp": timestamp,
            "metadata": metadata or {}
        })

        doc_id = f"{category}_{len(self.context['conversation_log'])}"
        self.search_engine.add_document(doc_id, content, metadata)

        self.save()

    def get_relevant_context(self, current_query: str,
                             max_tokens: int = 500) -> str:
        """è·å–ç›¸å…³ä¸Šä¸‹æ–‡ï¼ˆæ›¿ä»£åŠ è½½å…¨éƒ¨å†å²ï¼‰"""
        relevant = self.smart_recall(current_query, max_results=3)

        if not relevant:
            return "ï¼ˆæ— ç›¸å…³å†å²è®°å½•ï¼‰"

        parts = ["=== ç›¸å…³è®°å¿† ==="]
        current_tokens = 0

        for mem in relevant:
            text = f"[{mem['source']}] {mem['content'][:200]}"
            tokens = len(text) // 4
            if current_tokens + tokens > max_tokens:
                break
            parts.append(text)
            current_tokens += tokens

        return "\n".join(parts)

    def get_memory_stats(self) -> Dict:
        search_stats = self.search_engine.get_stats()
        return {
            **search_stats,
            "total_conversations": len(self.context["conversation_log"]),
            "active_tasks": len(self.context["tasks"]["active"]),
            "search_efficiency": {
                "total_searches": self.stats["searches"],
                "successful_hits": self.stats["hits"],
                "hit_rate": (
                    f"{self.stats['hits'] / self.stats['searches'] * 100:.1f}%"
                    if self.stats["searches"] > 0 else "0%"
                ),
                "estimated_tokens_saved": self.stats["token_saved"]
            }
        }

    def print_stats(self):
        stats = self.get_memory_stats()
        print("\n" + "=" * 60)
        print("ğŸ“Š å¢å¼ºç‰ˆè®°å¿†ç³»ç»Ÿ V3 ç»Ÿè®¡")
        print("=" * 60)
        print(f"ğŸ’¾ å­˜å‚¨ç»Ÿè®¡:")
        print(f"  - æ–‡æ¡£æ€»æ•°: {stats['total_documents']}")
        print(f"  - ç¼“å­˜å‘é‡: {stats['cached_vectors']}")
        print(f"  - å¯¹è¯æ¡æ•°: {stats['total_conversations']}")
        print(f"\nğŸ” æœç´¢æ•ˆç‡:")
        print(f"  - æœç´¢æ¬¡æ•°: {stats['search_efficiency']['total_searches']}")
        print(f"  - å‘½ä¸­ç‡: {stats['search_efficiency']['hit_rate']}")
        print(f"  - ä¼°ç®—èŠ‚çœ Token: {stats['search_efficiency']['estimated_tokens_saved']:,}")
        print(f"\nğŸŒ Embedding ä¾›åº”å•†:")
        for p in stats["embedding_provider"]["providers"]:
            status = "âœ…" if p["available"] else "âŒ"
            print(f"  {status} {p['name']} ({p['model']})")
        print("=" * 60 + "\n")


# ========== æ¼”ç¤º ==========

if __name__ == "__main__":
    print("ğŸš€ OpenClaw å¢å¼ºè®°å¿†ç³»ç»Ÿ V3 æ¼”ç¤º\n")

    memory = EnhancedMemoryCore(
        storage_path="/tmp/test_memory_v3.json",
        config_dir=os.path.dirname(os.path.abspath(__file__))
    )

    # æ·»åŠ è®°å¿†
    print("ğŸ“ æ·»åŠ å†å²è®°å¿†...")
    memory.add_memory("ç”¨æˆ·å–œæ¬¢ç®€æ´çš„ä»£ç é£æ ¼ï¼Œä¸å–œæ¬¢è¿‡å¤šæ³¨é‡Š", category="preference")
    memory.add_memory("æ­£åœ¨å¼€å‘ä¸€ä¸ªPythoné‡åŒ–äº¤æ˜“æœºå™¨äºº", category="project")
    memory.add_memory("è®¨è®ºäº†å¦‚ä½•ä¼˜åŒ–OpenClawçš„Tokenæ¶ˆè€—", category="conversation")
    memory.add_memory("ICLRè®ºæ–‡æˆªæ­¢æ—¥æœŸæ˜¯2026å¹´3æœˆ", category="task")
    print("âœ… è®°å¿†å·²æ·»åŠ \n")

    # å…³é”®æµ‹è¯•ï¼šè¯­ä¹‰æœç´¢ vs è¯è¢‹åŒ¹é…
    print("=" * 60)
    print("ğŸ§ª å…³é”®æµ‹è¯•ï¼šç”¨ä¸åŒçš„è¯æœç´¢ç›¸åŒå«ä¹‰")
    print("=" * 60)

    test_queries = [
        ("ç¼–ç¨‹è§„èŒƒ", "åº”åŒ¹é…'ä»£ç é£æ ¼'ï¼ˆè¯­ä¹‰ç›¸è¿‘ä½†è¯ä¸åŒï¼‰"),
        ("é‡åŒ–ç­–ç•¥", "åº”åŒ¹é…'é‡åŒ–äº¤æ˜“æœºå™¨äºº'"),
        ("è®ºæ–‡è¿›åº¦", "åº”åŒ¹é…'ICLRè®ºæ–‡æˆªæ­¢æ—¥æœŸ'"),
    ]

    for query, expected in test_queries:
        print(f"\nğŸ” æŸ¥è¯¢: '{query}' â€” {expected}")
        results = memory.smart_recall(query, max_results=2)
        for r in results:
            print(f"  [{r['score']:.4f}] {r['content'][:60]}...")

    # ç»Ÿè®¡
    print()
    memory.print_stats()
